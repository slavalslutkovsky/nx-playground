
services:
#  vault-test:
#    image: vault:latest
#    volumes:
#      - ./vault/config:/vault/config
#      - ./vault/policies:/vault/policies
#      - ./vault/data:/vault/data
#    ports:
#      - 8200:8200
#    environment:
#      - VAULT_ADDR=http://0.0.0.0:8200
#      - VAULT_API_ADDR=http://0.0.0.0:8200
#      - VAULT_ADDRESS=http://0.0.0.0:8200
#    cap_add:
#      - IPC_LOCK
#    command: vault server -config=/vault/config/vault.json
  vault:
    image: hashicorp/vault:latest
    container_name: vault
    restart: unless-stopped
#    healthcheck:
#      retries: 5
    environment:
      VAULT_ADDR: "https://127.0.0.1:8200"
      VAULT_API_ADDR: "https://127.0.0.1:8200"
      VAULT_LOCAL_CONFIG: |
        {
          "listener": [
            {
              "tcp": {
                "address": "0.0.0.0:8200",
                "tls_disable": 0,
                "tls_cert_file": "/vault/config/certs/vault-cert.pem",
                "tls_key_file": "/vault/config/certs/vault-key.pem"
              }
            }
          ],
          "storage": {
            "file": {
              "path": "/vault/data"
            }
          },
          "default_lease_ttl": "168h",
          "max_lease_ttl": "720h",
          "ui": true
        }
    ports:
      - "8200:8200"
    volumes:
      - "<CUSTOM_USER_DIRECTORY>/data:/vault/data"
      - "<CUSTOM_USER_DIRECTORY>/certs:/vault/config/certs"
    cap_add:
      - IPC_LOCK
    command: "vault server -config vault/config/local.json"
  mongo:
    image: "mongo"
    ports:
      - "27017:27017"
  redis:
    image: "redis:alpine"
    ports:
      - "6379:6379"

  nats:
    image: nats:2.10-alpine
    container_name: nats
    restart: always
    ports:
      - "4222:4222"    # Client connections
      - "8222:8222"    # HTTP monitoring
      - "6222:6222"    # Cluster routing (optional)
    command: ["--jetstream", "-m", "8222"]
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:8222/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
  postgres:
    image: "postgres"
    environment:
      POSTGRES_DB: mydatabase
      POSTGRES_USER: myuser
      POSTGRES_PASSWORD: mypassword
      # Performance tuning for development/benchmarking
      # Adjust based on your system's available RAM
      POSTGRES_SHARED_BUFFERS: "512MB"           # 25% of RAM for caching (default: 128MB)
      POSTGRES_EFFECTIVE_CACHE_SIZE: "2GB"       # 50-75% of RAM (planner hint)
      POSTGRES_WORK_MEM: "16MB"                  # Per-query operation memory (default: 4MB)
      POSTGRES_MAINTENANCE_WORK_MEM: "128MB"     # VACUUM, CREATE INDEX memory (default: 64MB)
      POSTGRES_MAX_CONNECTIONS: "200"            # Match application pool (default: 100)
      POSTGRES_WAL_BUFFERS: "16MB"               # WAL buffer size (default: -1)
      POSTGRES_CHECKPOINT_COMPLETION_TARGET: "0.9" # Spread checkpoint I/O (default: 0.5)
      POSTGRES_RANDOM_PAGE_COST: "1.1"           # Lower for SSD (default: 4.0)
      POSTGRES_EFFECTIVE_IO_CONCURRENCY: "200"   # SSD concurrent I/O (default: 1)
    ports:
      - "5432:5432"
    command:
      - "postgres"
      - "-c"
      - "shared_buffers=512MB"
      - "-c"
      - "effective_cache_size=2GB"
      - "-c"
      - "work_mem=16MB"
      - "-c"
      - "maintenance_work_mem=128MB"
      - "-c"
      - "max_connections=200"
      - "-c"
      - "wal_buffers=16MB"
      - "-c"
      - "checkpoint_completion_target=0.9"
      - "-c"
      - "random_page_cost=1.1"
      - "-c"
      - "effective_io_concurrency=200"
      - "-c"
      - "min_wal_size=1GB"
      - "-c"
      - "max_wal_size=4GB"
  mailhog:
    image: mailhog/mailhog
    ports:
      - "1025:1025"
      - "8025:8025"

  influxdb2:
    image: influxdb:2
    ports:
      - "8086:8086"
    environment:
      DOCKER_INFLUXDB_INIT_MODE: setup
      DOCKER_INFLUXDB_INIT_USERNAME: admin
      DOCKER_INFLUXDB_INIT_PASSWORD: strongpassword
      DOCKER_INFLUXDB_INIT_ADMIN_TOKEN: admintoken123
      DOCKER_INFLUXDB_INIT_ORG: docs
      DOCKER_INFLUXDB_INIT_BUCKET: home

  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    restart: always
    ports:
      - "6333:6333"    # REST API
      - "6334:6334"    # gRPC API (optional)
    volumes:
      #      - qdrant_data:/qdrant/storage
      - ./config:/qdrant/config
    environment:
      #      - QDRANT__SERVICE__API_KEY=your-secret-key-here
      - QDRANT__LOG_LEVEL=INFO
      - QDRANT__SERVICE__HTTP_PORT=6333
      - QDRANT__SERVICE__GRPC_PORT=6334
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:6333/healthz" ]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - qdrant-network

  neo4j:
    image: neo4j:5-community
    container_name: neo4j
    restart: always
    ports:
      - "7474:7474"    # HTTP (Browser)
      - "7687:7687"    # Bolt protocol
    environment:
      - NEO4J_AUTH=neo4j/password123
      - NEO4J_PLUGINS=["apoc"]
      - NEO4J_dbms_security_procedures_unrestricted=apoc.*
      - NEO4J_dbms_memory_heap_initial__size=512m
      - NEO4J_dbms_memory_heap_max__size=1G
    volumes:
      - neo4j_data:/data
      - neo4j_logs:/logs
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7474"]
      interval: 30s
      timeout: 10s
      retries: 3

  arangodb:
    image: arangodb:3.11
    container_name: arangodb
    restart: always
    ports:
      - "8529:8529"    # Web UI and REST API
    environment:
      - ARANGO_ROOT_PASSWORD=rootpassword
      - ARANGO_NO_AUTH=0
    volumes:
      - arangodb_data:/var/lib/arangodb3
      - arangodb_apps:/var/lib/arangodb3-apps
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8529/_api/version"]
      interval: 30s
      timeout: 10s
      retries: 3

  milvus-etcd:
    image: quay.io/coreos/etcd:v3.5.5
    container_name: milvus-etcd
    environment:
      - ETCD_AUTO_COMPACTION_MODE=revision
      - ETCD_AUTO_COMPACTION_RETENTION=1000
      - ETCD_QUOTA_BACKEND_BYTES=4294967296
      - ETCD_SNAPSHOT_COUNT=50000
    volumes:
      - milvus_etcd:/etcd
    command: etcd -advertise-client-urls=http://127.0.0.1:2379 -listen-client-urls http://0.0.0.0:2379 --data-dir /etcd
    healthcheck:
      test: ["CMD", "etcdctl", "endpoint", "health"]
      interval: 30s
      timeout: 20s
      retries: 3

#  milvus-minio:
#    image: minio/minio:RELEASE.2023-03-20T20-16-18Z
#    container_name: milvus-minio
#    environment:
#      MINIO_ACCESS_KEY: minioadmin
#      MINIO_SECRET_KEY: minioadmin
#    ports:
#      - "9001:9001"    # MinIO Console
#      - "9000:9000"    # MinIO API
#    volumes:
#      - milvus_minio:/minio_data
#    command: minio server /minio_data --console-address ":9001"
#    healthcheck:
#      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
#      interval: 30s
#      timeout: 20s
#      retries: 3
#
#  milvus:
#    image: milvusdb/milvus:v2.3.3
#    container_name: milvus
#    restart: always
#    command: ["milvus", "run", "standalone"]
#    security_opt:
#      - seccomp:unconfined
#    environment:
#      ETCD_ENDPOINTS: milvus-etcd:2379
#      MINIO_ADDRESS: milvus-minio:9000
#    volumes:
#      - milvus_data:/var/lib/milvus
#    ports:
#      - "19530:19530"  # gRPC
#      - "9091:9091"    # Metrics
#      - "19121:19121"  # REST API (Attu compatible)
#    depends_on:
#      - milvus-etcd
#      - milvus-minio
#    healthcheck:
#      test: ["CMD", "curl", "-f", "http://localhost:9091/healthz"]
#      interval: 30s
#      timeout: 20s
#      retries: 3

#  rabbitmq:
#    image: rabbitmq:3-management
#    container_name: rabbitmq
#    restart: always
#    ports:
#      - "5672:5672"    # AMQP
#      - "15672:15672"  # Management UI
#      - "9419:9419"    # Prometheus metrics
#    environment:
#      - RABBITMQ_DEFAULT_USER=admin
#      - RABBITMQ_DEFAULT_PASS=admin
#      - RABBITMQ_PROMETHEUS_RETENTION_POLICY=24h
#    volumes:
#      - rabbitmq_data:/var/lib/rabbitmq
#  nushell:
#    image: ghcr.io/nushell/nushell:latest-alpine
#    command: |
#      ls
#      sleep 29
#  prometheus:
#    image: prom/prometheus:latest
#    container_name: prometheus
#    restart: always
#    volumes:
#      - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro
#    ports:
#      - "9090:9090"
#  loki:
#    image: grafana/loki:latest
#    container_name: loki
#    restart: always
#    ports:
#      - "3100:3100"
#    volumes:
#      - ./loki-config.yml:/etc/loki/local-config.yaml
#  grafana:
#    image: grafana/grafana:latest
#    container_name: grafana
#    environment:
#      - GF_SECURITY_ADMIN_PASSWORD=admin
#    ports:
#      - "3000:3000"
#    depends_on:
#      - prometheus
#      - loki
#    volumes:
#      - influxdb2-data:/var/lib/influxdb2
#      - influxdb2-config:/etc/influxdb2
volumes:
  influxdb2-data:
  influxdb2-config:
  qdrant_data:
  rabbitmq_data:
  neo4j_data:
  neo4j_logs:
  arangodb_data:
  arangodb_apps:
  milvus_etcd:
  milvus_minio:
  milvus_data:
networks:
  qdrant-network:
    driver: bridge

# JetBrains
# jdbc:postgresql://localhost:5432/mydatabase?user=myuser&password=mypassword
